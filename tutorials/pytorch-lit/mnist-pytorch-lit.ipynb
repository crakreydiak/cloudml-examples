{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequesites\n",
    "1. An Azure subscription. If you don't have on, you can [create a free account](https://aka.ms/AMLFree).\n",
    "2. A working Azure ML Workspace that can be created via Azure Portal, Azure CLI or Python SDK.\n",
    "3. An Azure Machine Learning workspace.\n",
    "\n",
    "The following step helps you set up [2]() and [3](). If these resources already exist, you can skip this block. Visit [this link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace-cli?tabs=createnewresources) for more information on how to manage your AML workspace using Azure CLI.\n",
    "\n",
    "In the following example, a new workspace named `mnist-pytorch-lit-ws` is attached to the new resource group `ideas-rg` located in `east-us`. We add tags to easily remove the created resources at the end of this tutorial. The shell used is powershell, but the same command can be executed in bash for MacOS and Linux users. To install Azure CLI, [click here](https://learn.microsoft.com/en-us/cli/azure/) and follow the instructions.\n",
    "\n",
    "```powershell\n",
    "# add the \"azure-cli-ml\" extension\n",
    "az extension add --name azure-cli-ml`\n",
    "# login to azure (will open a browser for authentification)\n",
    "az login\n",
    "# create a resource group named \"ideas-rg\"\n",
    "az group create --name ideas-rg --location eastus --tags env=tutorial\n",
    "# create a workspace named \"mnist-pytorch-lit-ws\" attached to resource group \"mnist-pytorch-lit\"\n",
    "az ml workspace create -w mnist-pytorch-lit-ws -g ideas-rg --tags env=tutorial\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**:\n",
    "> When you deploy an Azure Machine Learning workspace, various [other services are created by default](https://learn.microsoft.com/en-us/azure/machine-learning/concept-workspace#associated-resources). Theses services include an [Azure Storage Account](https://azure.microsoft.com/en-us/products/category/storage/) used to store models, checkpoints and our data. Depending on your needs, you might want to create the storage account ahead of time and link it to the workspace. For ML workflows, Microsoft recommands [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs/), the \"massively scalable and secure object storage\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your Azure Workspace\n",
    "\n",
    "Connecting to your workspace is rather simple. We need to instantiate a `azure.ai.ml.MLClient` object, authenticate ourself and identify the subscription, resource group and workspace. We use `DefaultAzureCredential` to gain access to the workspace but alternatively the `InteractiveBrowserCredential` can be used if you prefer using a browser to authenticate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace client\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentification package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "cred = DefaultAzureCredential()\n",
    "# uncomment to use the InteractiveBrowserCredential instead\n",
    "# from azure.identity import InteractiveBrowserCredential\n",
    "# creds = InteractiveBrowserCredential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your subscription ID, the names of your resource group and workspace, we can now create a workspace handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = \"<YOUR_SUBSCRIPTION_ID>\" # replace with your own value, which can be accessed from Azure Portal\n",
    "azure_RG = \"ideas-rg\" # matches the name of the resource group we created\n",
    "azure_WS = \"mnist-pytorch-lit-ws\"  # matches the name of the workspace we created\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=cred,\n",
    "    subscription_id=sub_id,\n",
    "    resource_group_name=azure_RG,\n",
    "    workspace_name=azure_WS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute resources to run jobs\n",
    "Cloud-based tasks are specified by jobs that run on various VMs. In this tutorial, we'll need two clusters: a basic CPU to upload our local data and a GPU cluster to run the training job. Multiple choice of architectures [are available here](https://azure.microsoft.com/en-us/pricing/details/machine-learning/) but beware of subscription restrictions that can limit your options to basic VMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "# Checks if the resource exists and creates it if not.\n",
    "# An `if` statement would be better, but since there is currently no `exists` method, \n",
    "# we need this try/catch statement \n",
    "try: \n",
    "    # throw an exception if the resource does not exist\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(f\"Found existing cluster {cpu_compute_target}.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    cpu_cluster = AmlCompute(\n",
    "        # Name of the cluster\n",
    "        name=cpu_compute_target,\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    # Create the cluster\n",
    "    cpu_cluster = ml_client.begin_create_or_update(cpu_cluster).result()\n",
    "\n",
    "print(f\"AMLCompute with name {cpu_cluster.name} is used with compute size {cpu_cluster.size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GPUs, we create the cluster from the smallest possible VM family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "gpu_compute_target = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    gpu_cluster = ml_client.compute.get(gpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {gpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new gpu compute target...\")\n",
    "\n",
    "    gpu_cluster = AmlCompute(\n",
    "        name=\"gpu-cluster\",\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_NC6\",  # 1 x NVIDIA Tesla K80 ($0.90 per)\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=180,\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {gpu_cluster.name} is created with the compute size {gpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload the data\n",
    "\n",
    "We train a basic PyTorch Lightning AutoEncoder on the well-known MNIST dataset. We could download the dataset using a public URL during training, but this would increase the time usage of our most expensive hardware. Alternatively, we can submit a job to download the data from the same URL. In production, we would probably skip this step entirely since the data will already be stored somewhere in Azure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training and test data \n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import os\n",
    "\n",
    "# create local folder to store data\n",
    "os.makedirs(\"./data\", exist_ok=\"ok\")\n",
    "\n",
    "# download training data\n",
    "train_data = MNIST(\n",
    "    \"./data/train\",\n",
    "    train=True, download=True,\n",
    ")\n",
    "# download test data\n",
    "test_data = MNIST(\n",
    "    \"./data/test\",\n",
    "    train=False, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# create archive\n",
    "!tar -czvf mnist.tar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# we define the command here and submit it in the next block\n",
    "upload_dataset_job = command(\n",
    "    display_name=\"upload_mnist\",\n",
    "    command=\"tar xvfm ${{inputs.archive}} --no-same-owner -C ${{outputs.images}}\",\n",
    "    inputs={\n",
    "        \"archive\": Input(\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            path=\"./mnist.tar\"\n",
    "        )\n",
    "    },\n",
    "    outputs={\n",
    "        \"images\": Output(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            mode=\"upload\",\n",
    "            path=\"azureml://datastores/workspaceblobstore/paths/mnist-pytorch-lit-tutorial/data/\"\n",
    "        )\n",
    "    },\n",
    "    # an existing environment with pre-installed libraries\n",
    "    # we can create our own for the current purposes we can re-use an existing one\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    # the cpu compute resource we just created\n",
    "    compute=cpu_compute_target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(\n",
    "    upload_dataset_job,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.studio_url)\n",
    "# open the browser with this url\n",
    "webbrowser.open(returned_job.studio_url)\n",
    "\n",
    "# print the pipeline run id\n",
    "print(\n",
    "    f\"The pipeline details can be access programmatically using identifier: {returned_job.name}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a custom training environment \n",
    "Our training job requires specific dependencies identified in `env.yaml`. To load these libraries, Azure provides [environments](https://docs.microsoft.com/azure/machine-learning/concept-environments). An environment lists the software runtime and libraries that you want installed on the compute where youâ€™ll be training. It's similar to your python environment on your local machine.\n",
    "\n",
    "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
    "\n",
    "In this example, we use a custom conda environment for the training job, using a conda yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os \n",
    "\n",
    "custom_env_name = \"mnist-pytorch-lit-env\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for the MNIST PytorchLit tutorial\",\n",
    "    tags={\"pytorch\": \"1.13\", \"cuda\": \"11.7\"},\n",
    "    conda_file=\"env.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"\"\"Environment with name {pipeline_job_env.name} is registered to workspace,\n",
    "     the environment version is {pipeline_job_env.version}\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and submit the training command job\n",
    "Using the environment created in the previous step, we can now define the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "experiment_name = \"train_mnist_litautoencoder\"\n",
    "\n",
    "training_job = command(\n",
    "    # local path to the source code\n",
    "    code=\"./src/\",\n",
    "    # the commands to run the python script\n",
    "    command=\"\"\"python train.py \\\n",
    "        --path_to_data ${{inputs.path_to_data}} \\\n",
    "        --batch_size ${{inputs.batch_size}} \\\n",
    "        --max_epochs ${{inputs.max_epochs}} \\\n",
    "        --num_workers ${{inputs.num_workers}} \\\n",
    "        --hidden_dim ${{inputs.hidden_dim}}\n",
    "    \"\"\",\n",
    "    # inject variables to the command above\n",
    "    inputs={\n",
    "        \"path_to_data\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=\"azureml://datastores/workspaceblobstore/paths/mnist-pytorch-lit-tutorial/data\",\n",
    "            mode=\"download\" # use `download` to make access faster, `mount` if dataset is larger than VM\n",
    "        ),\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 25,\n",
    "        \"num_workers\": 4,\n",
    "        \"hidden_dim\": 3,\n",
    "    },\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        # set process count to the number of gpus on the node\n",
    "        # NC6 has only 1\n",
    "        \"process_count_per_instance\": 1\n",
    "    },\n",
    "    # you can create multiple versions of the same environment, use @latest to fetch the latest one\n",
    "    environment=f\"{custom_env_name}@latest\",\n",
    "    # the name of compute infrastructure needed\n",
    "    compute=gpu_compute_target,\n",
    "    # set instance count to the number of nodes you want to use (1 * 6vCPUs)\n",
    "    # ***to use more resources, you will need to increase your quotas***\n",
    "    # https://learn.microsoft.com/en-us/azure/quotas/per-vm-quota-requests\n",
    "    instance_count=1,\n",
    "    # full name of the experiment (optional)\n",
    "    experiment_name=experiment_name,\n",
    "    # friendly name displayed in tables (option)\n",
    "    display_name=\"train_mnist_litautoencoder\",\n",
    "    description=\"training an autoencoder on mnist dataset\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    training_job,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.studio_url)\n",
    "# open the browser with this url\n",
    "webbrowser.open(returned_job.studio_url)\n",
    "\n",
    "# print the pipeline run id\n",
    "print(\n",
    "    f\"The pipeline details can be access programmatically using identifier: {returned_job.name}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**:\n",
    "> Your default subscription might be limited to 6 vCPUs. To increase your quotas, follow the official instructions [here](https://learn.microsoft.com/en-us/azure/quotas/per-vm-quota-requests)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57d70bcfee0b486d3a4c02f1b522b98497ced4e081d36c3d50cea0305f7db47e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
